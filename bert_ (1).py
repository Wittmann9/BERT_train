# -*- coding: utf-8 -*-
"""bert_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xl3pwdL1zzuzTrMeIgUCW9NLwnkQoahy
"""

from torch.utils.data.dataloader import DataLoader
import nltk
import pandas as pd
import string
import torch 
from torch.utils.data import Dataset, DataLoader
from torchvision import datasets

!pip install transformers datasets

from transformers import AutoTokenizer, AutoModel

from google.colab import drive
drive.mount('/content/drive')

dataset = pd.read_csv('/content/ArSarcasm_train.csv')
dataset.head()

class CustomDataset(Dataset):
  def __init__(self, data):
    self.data = data
    self.data['labels']  = dataset['sarcasm'].apply(lambda x :int(x))

  def __len__(self):
    return len(self.data)

  def __getitem__(self, idx):
    tweet = self.data.iloc[idx]['tweet']
    label = self.data.iloc[idx]['labels']
    return tweet, label

class LoadingData:
  def __init__(self, tokenizer, data, batch_size = 64, shuffle = True, drop_last = True, num_workers=2, device = 'cpu'):
    self.dataloader = DataLoader(data, batch_size=batch_size, shuffle = shuffle, 
                                 drop_last = drop_last, num_workers=num_workers, collate_fn=self.collate_fn)
    self.tokenizer = tokenizer
    self.device = device
  def collate_fn(self, batch):
    list_of_texts = [x[0] for x in batch]
    list_of_labels = [x[1] for x in batch]
    labels = torch.Tensor(list_of_labels).long()
    tokenized_text = self.tokenizer(list_of_texts, 
                                return_tensors="pt",
                                truncation=True,
                                padding=True)
    tokenized_text  = {k : v.to(self.device) for k, v in tokenized_text.items()}
    return tokenized_text, labels

import torch.nn as nn

# model = AutoModel.from_pretrained("distilbert-base-uncased")

from torch.nn.modules.activation import Softmax


class BertMeanClassifier(nn.Module):
    def __init__(self, transformer_name = 'distilbert-base-uncased', 
                 n_classes =2, num_hids_mean= 4):
        super().__init__()
        self.transformer = AutoModel.from_pretrained(transformer_name, 
                                                      output_hidden_states = True)
        self.scorer = nn.Sequential(
            nn.Linear(768, n_classes),
            nn.Softmax(dim=-1) 
        )
        self.num_hids_mean = num_hids_mean

    def _mean_hidden_states(self, output):
      print(len(output))
      last_n_hinned = output[-self.num_hids_mean:]
      print(len(last_n_hinned))
      cls_represent = [x[:,0,:] for x in last_n_hinned]
      print(len(cls_represent))
      cls_representation = torch.stack(cls_represent, dim = 1)
      print(cls_representation.shape)
      cls_means = torch.mean(cls_representation, dim = 1)
      print(cls_means.shape)
      return cls_means

    def forward(self, tokenized_inputs):
        trans_output = self.transformer(**tokenized_inputs).hidden_states
        mean_hiddens = self._mean_hidden_states(trans_output)
        scores = self.scorer(mean_hiddens)
        return scores

# torch.randn(10,20,15)[:,0,:].shape
# a = torch.rand(4, 4, 3)
# c = torch.rand(4, 4, 3)
# d = (a, c)
# b = torch.stack(d, dim = 1)
# b.shape
# k = torch.mean(b, dim = 1)
# # k.shape
# k

args_dict = {
    'transformer_name' : 'aubmindlab/bert-base-arabertv02-twitter',
    'num_hids_mean': 4,
    'n_classes': 2
}
tokenizer = AutoTokenizer.from_pretrained(args_dict['transformer_name'])

# args_dict = {
#     'transformer_name' : 'aubmindlab/bert-base-arabertv02-twitter',
#     'num_hids_mean': 4,
#     'n_classes': 2
# }
# tokenizer = AutoTokenizer.from_pretrained(args_dict['transformer_name'])

tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02-twitter')

model = BertMeanClassifier(**args_dict)

processed_dataset = CustomDataset(dataset)

data_loaded = LoadingData(
     tokenizer = tokenizer,
     data = processed_dataset, 
     batch_size=4
)

data_loaded.dataloader

import datasets
classification_metrics = datasets.load_metric("accuracy")
f_1_metric = datasets.load_metric("f1")
# f1_metric = datasets.load_metric("f1")

for tok_inputs, labels in data_loaded.dataloader:
  # print(tok_inputs)
  # print(labels)
  
  scores = model(tok_inputs)

  classification_metrics.add_batch(predictions=torch.argmax(scores, dim = -1), references=labels)
  f_1_metric.add_batch(predictions=torch.argmax(scores, dim = -1), references=labels)
final_score = classification_metrics.compute()
f_1_score = f_1_metric.compute()
f_1_score
final_score

      # print(len(output))
      # last_n_hinned = output[-self.num_hids_mean:]
      # print(len(last_n_hinned))
      # cls_represent = [x[:,0,:] for x in last_n_hinned]
      # print(len(cls_represent))
      # cls_representation = torch.stack(cls_represent, dim = 1)
      # print(cls_representation.shape)
      # cls_means = torch.mean(cls_representation, dim = 1)
      # print(cls_means.shape)

scores

# !git clone https://github.com/aub-mind/arabert
# os.getcwd()

# from arabert.preprocess import ArabertPreprocessor, never_split_tokens

model_name="aubmindlab/bert-base-arabertv02"
# arabert_prep = ArabertPreprocessor(model_name=model_name)
model  = AutoModel.from_pretrained(model_name)

text = "ولن نبالغ إذا قلنا إن هاتف أو كمبيوتر المكتب في زمننا هذا ضروري"
model(text)

