# -*- coding: utf-8 -*-
"""bert_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xl3pwdL1zzuzTrMeIgUCW9NLwnkQoahy
"""

from torch.utils.data.dataloader import DataLoader
import nltk
import pandas as pd
import numpy as np
# import string
import torch 
from torch.utils.data import Dataset, DataLoader
# from torchvision import datasets
from torch.nn.modules.activation import Softmax
from transformers import AutoModelForSequenceClassification
from torch.optim import AdamW
from transformers import get_scheduler
from transformers import TrainingArguments
from torch.utils.tensorboard import SummaryWriter
import datasets
import os

accuracy_metric = datasets.load_metric("accuracy")
f1_metric = datasets.load_metric("f1")

accuracy_for_class_0 = datasets.load_metric("accuracy")
accuracy_for_class_1 = datasets.load_metric("accuracy")


torch.manual_seed(69)

from transformers import AutoTokenizer, AutoModel,  TrainingArguments, Trainer


import json

class CustomDataset(Dataset):
  def __init__(self, data):
    self.data = data
    self.data['labels']  = data['sarcasm'].apply(lambda x :int(x))

  def __len__(self):
    return len(self.data)

  def __getitem__(self, idx):
    tweet = self.data.iloc[idx]['tweet']
    label = self.data.iloc[idx]['labels']
    return tweet, label

class LoadingData:
  def __init__(self, tokenizer, data, batch_size = 64, shuffle = True, drop_last = True, num_workers=2, device = 'cpu'):
    self.dataloader = DataLoader(data, batch_size=batch_size, shuffle = shuffle, 
                                 drop_last = drop_last, num_workers=num_workers, collate_fn=self.collate_fn)
    self.tokenizer = tokenizer
    self.device = device
  def collate_fn(self, batch):
    list_of_texts = [x[0] for x in batch]
    list_of_labels = [x[1] for x in batch]
    labels = torch.Tensor(list_of_labels).long()
    tokenized_text = self.tokenizer(list_of_texts, 
                                return_tensors="pt",
                                truncation=True,
                                padding=True)
    tokenized_text  = {k : v.to(self.device) for k, v in tokenized_text.items()}
    return tokenized_text, labels

import torch.nn as nn

# model = AutoModel.from_pretrained("distilbert-base-uncased")




class BertMeanClassifier(nn.Module):
    def __init__(self, transformer_name = 'distilbert-base-uncased', 
                 n_classes =2, num_hids_mean= 4):
        super().__init__()
        self.transformer = AutoModel.from_pretrained(transformer_name, 
                                                      output_hidden_states = True)
        self.scorer = nn.Sequential(
            nn.Linear(768, n_classes),
            nn.Softmax(dim=-1) 
        )
        self.num_hids_mean = num_hids_mean

    def _mean_hidden_states(self, output):
      # print(len(output))
      last_n_hinned = output[-self.num_hids_mean:]
      # print(len(last_n_hinned))
      cls_represent = [x[:,0,:] for x in last_n_hinned]
      # print(len(cls_represent))
      cls_representation = torch.stack(cls_represent, dim = 1)
      # print(cls_representation.shape)
      cls_means = torch.mean(cls_representation, dim = 1)
      # print(cls_means.shape)
      return cls_means

    def forward(self, tokenized_inputs):
        trans_output = self.transformer(**tokenized_inputs).hidden_states
        mean_hiddens = self._mean_hidden_states(trans_output)
        scores = self.scorer(mean_hiddens)
        return scores



if __name__ == '__main__':
    save_dir = 'arab_bert_baseline'

    tokenizer = AutoTokenizer.from_pretrained('aubmindlab/bert-base-arabertv02-twitter')

    train_dataset = pd.read_csv('datasets/ArSarcasm_train.csv')
    test_dataset = pd.read_csv('datasets/sarcasm_test.csv')

    train_data = CustomDataset(train_dataset)
    test_data = CustomDataset(test_dataset)


    train_dataloader = LoadingData(
         tokenizer = tokenizer,
         data = train_data,
         num_workers=1,
         batch_size=4
    ).dataloader

    test_dataloader = LoadingData(
         tokenizer = tokenizer,
         data = test_data,
         num_workers=1,
         batch_size=4
    ).dataloader

    args_dict = {
        'transformer_name' : 'aubmindlab/bert-base-arabertv02-twitter',
        'num_hids_mean': 4,
        'n_classes': 2
    }
    # LR = 5e-10
    LR = 5e-3

    model = BertMeanClassifier(**args_dict)
    optimizer = AdamW(model.parameters(), lr=LR)
    criterion = torch.nn.CrossEntropyLoss()

    num_epochs = 3
    num_training_steps = num_epochs * len(train_dataloader)
    lr_scheduler = get_scheduler(
        name = 'linear', optimizer=optimizer, num_warmup_steps=0, num_training_steps = num_training_steps
    )

    device = "cpu"
    model.to(device)

    from tqdm.auto import tqdm

    progress_bar = tqdm(range(num_training_steps))
    writer = SummaryWriter(os.path.join(save_dir, 'tensorBoarg_logs'))

    def testing_metrics(test_dataloader, model):
        model.eval()
        metrics_dict = {}
        for tokenized_text, labels in test_dataloader:
            tokenized_text = {k: v.to(device) for k, v in tokenized_text.items()}
            with torch.no_grad():
                logits = model(tokenized_text)

            predictions = torch.argmax(logits, dim=-1)
            idx_0 = labels == 0
            idx_1 = labels == 1

            accuracy_metric.add_batch(predictions=predictions, references=labels)
            f1_metric.add_batch(predictions=predictions, references=labels)
            accuracy_for_class_0.add_batch(predictions = predictions[idx_0], references = [0]*sum(idx_0))
            accuracy_for_class_1.add_batch(predictions = predictions[idx_1], references = [1]*sum(idx_1))

        metrics_dict.update(accuracy_metric.compute())
        metrics_dict.update(f1_metric.compute())

        accuracy_0 = accuracy_for_class_0.compute()
        accuracy_0 = {k + '_0':v for k, v in accuracy_0.items()}

        accuracy_1 = accuracy_for_class_1.compute()
        accuracy_1 = {k + '_1': v for k, v in accuracy_1.items()}

        metrics_dict.update(accuracy_0)
        metrics_dict.update(accuracy_1)
        return metrics_dict

    def save_checkpoints(directory, num_iter, model, optimizer):
        torch.save({
            'num_iter': num_iter,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
        }, os.path.join(directory, f"checkpoint_{str(num_iter)}.pt"))

    def print_weight(model):
        for name, param in model.named_parameters():
            print(name)
            print(param)
            break

    i = 0
    for epoch in range(num_epochs):
        model.train()
        for tokenized_text, labels in train_dataloader:
            tokenized_text = {k: v.to(device) for k, v in tokenized_text.items()}
            outputs = model(tokenized_text)
            loss = criterion(outputs, labels)
            writer.add_scalar('Loss/train', loss.item(), i)
            loss.backward()

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)
            i+=1
            # print_weight(model)

            if i % 500 == 0:
                save_checkpoints(save_dir, i, model, optimizer)

        eval_metrics = testing_metrics(test_dataloader, model)
        for k, v in eval_metrics.items():
            writer.add_scalar(f'test/{k}', v, epoch)

















